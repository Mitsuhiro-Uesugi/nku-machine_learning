{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1030e1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7334e04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu():\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return np.maximum(self.x, 0)\n",
    "\n",
    "    def backward(self, eta):\n",
    "        eta[self.x <= 0] = 0\n",
    "        return eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22bf5a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sigmoid():\n",
    "    def forward(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def backward(self, eta):\n",
    "        return eta * (self.out * (1-self.out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8b5c832c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv():\n",
    "    \n",
    "    def __init__(self, filter_shape, stride=1, padding='SAME', bias=True, requires_grad=True):\n",
    "        self.weight = parameter(np.random.randn(*filter_shape) * (2/reduce(lambda x,y:x*y, filter_shape[1:]))**0.5)  \n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.requires_grad = requires_grad\n",
    "        self.output_channel = filter_shape[0]   \n",
    "        self.input_channel = filter_shape[1]    \n",
    "        self.filter_size = filter_shape[2]  \n",
    "        if bias:\n",
    "            self.bias = parameter(np.random.randn(self.output_channel))\n",
    "        else:\n",
    "            self.bias =None\n",
    "\n",
    "    def image_to_col(self, x, filter_size_x, filter_size_y, stride):\n",
    "        N, C, H, W = x.shape\n",
    "        output_H, output_W = (H-filter_size_x)//stride + 1, (W-filter_size_y)//stride + 1\n",
    "        out_size = output_H * output_W\n",
    "        x_cols = np.zeros((out_size*N, filter_size_x*filter_size_y*C))\n",
    "        for i in range(0, H-filter_size_x+1, stride):\n",
    "            i_start = i * output_W\n",
    "            for j in range(0, W-filter_size_y+1, stride):\n",
    "                temp = x[:,:, i:i+filter_size_x, j:j+filter_size_y].reshape(N,-1)\n",
    "                x_cols[i_start+j::out_size, :] = temp\n",
    "        return x_cols\n",
    "    \n",
    "    def forward(self, input):\n",
    "\n",
    "        # 边缘填充\n",
    "        if self.padding == \"VALID\":\n",
    "            self.x = input\n",
    "        if self.padding == \"SAME\":\n",
    "            p = self.filter_size // 2\n",
    "            self.x = np.lib.pad(input, ((0,0),(0,0),(p,p),(p,p)), \"constant\")\n",
    "        # 处理不能恰好的被卷积核的大小和选定的步长所整除的宽和高\n",
    "        x_fit = (self.x.shape[2] - self.filter_size) % self.stride\n",
    "        y_fit = (self.x.shape[3] - self.filter_size) % self.stride\n",
    "\n",
    "        if self.stride > 1:\n",
    "            if x_fit != 0:\n",
    "                self.x = self.x[:, :, 0:self.x.shape[2] - x_fit, :]\n",
    "            if y_fit != 0:\n",
    "                self.x = self.x[:, :, :, 0:self.x.shape[3] - y_fit]\n",
    "\n",
    "        N, _, H, W = self.x.shape\n",
    "        O, C, K, K = self.weight.data.shape\n",
    "        weight_cols = self.weight.data.reshape(O, -1).T\n",
    "        x_cols = self.image_to_col(self.x, self.filter_size, self.filter_size, self.stride)\n",
    "        result = np.dot(x_cols, weight_cols) + self.bias.data\n",
    "        output_H, output_W = (H-self.filter_size)//self.stride + 1, (W-self.filter_size)//self.stride + 1\n",
    "        result = result.reshape((N, result.shape[0]//N, -1)).reshape((N, output_H, output_W, O))\n",
    "        return result.transpose((0, 3, 1, 2))\n",
    "\n",
    "\n",
    "    def backward(self, eta, lr):\n",
    "\n",
    "        if self.stride > 1:\n",
    "            N, O, output_H, output_W = eta.shape\n",
    "            inserted_H, inserted_W = output_H + (output_H-1)*(self.stride-1), output_W + (output_W-1)*(self.stride-1)\n",
    "            inserted_eta = np.zeros((N, O, inserted_H, inserted_W))\n",
    "            inserted_eta[:,:,::self.stride, ::self.stride] = eta\n",
    "            eta = inserted_eta\n",
    "\n",
    "        N, _, output_H, output_W = eta.shape\n",
    "        self.b_grad = eta.sum(axis=(0,2,3))\n",
    "        self.W_grad = np.zeros(self.weight.data.shape)      \n",
    "        for i in range(self.filter_size):\n",
    "            for j in range(self.filter_size):\n",
    "                self.W_grad[:,:,i,j] = np.tensordot(eta, self.x[:,:,i:i+output_H,j:j+output_W], ([0,2,3], [0,2,3]))\n",
    "        self.weight.data -= lr * self.W_grad / N\n",
    "        self.bias.data -= lr * self.b_grad / N\n",
    "\n",
    "\n",
    "        if self.padding == \"VALID\":\n",
    "            p = self.filter_size - 1\n",
    "            pad_eta = np.lib.pad(eta, ((0,0),(0,0),(p,p),(p,p)), \"constant\", constant_values=0)\n",
    "            eta = pad_eta\n",
    "        elif self.padding == \"SAME\":\n",
    "            p = self.filter_size // 2\n",
    "            pad_eta = np.lib.pad(eta, ((0, 0), (0, 0), (p, p), (p, p)), \"constant\", constant_values=0)\n",
    "            eta = pad_eta\n",
    "\n",
    "        _, C, _, _ = self.weight.data.shape\n",
    "        weight_flip = np.flip(self.weight.data, (2,3))  \n",
    "        weight_flip_swap = np.swapaxes(weight_flip, 0, 1)  \n",
    "        weight_flip = weight_flip_swap.reshape(C, -1).T\n",
    "        x_cols = self.image_to_col(eta, self.filter_size, self.filter_size, self.stride)\n",
    "        result = np.dot(x_cols, weight_flip)\n",
    "        N, _, H, W = eta.shape\n",
    "        output_H, output_W = (H - self.filter_size) // self.stride + 1, (W - self.filter_size) // self.stride + 1\n",
    "        result = result.reshape((N, result.shape[0] // N, -1)).reshape((N, output_H, output_W, C))\n",
    "        self.weight.grad = result.transpose((0, 3, 1, 2))\n",
    "\n",
    "        return self.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8231605f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout():\n",
    "    def __init__(self, drop_rate=0.5, is_train=True):\n",
    "        self.drop_rate = drop_rate\n",
    "        self.is_train = is_train\n",
    "        self.fix_value = 1 - drop_rate   \n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.is_train==False:    \n",
    "            return x\n",
    "        else:            \n",
    "            N, m = x.shape\n",
    "            self.save_mask = np.random.uniform(0, 1, m) > self.drop_rate   \n",
    "            return (x * self.save_mask) / self.fix_value\n",
    "\n",
    "\n",
    "    def backward(self, eta):\n",
    "        if self.is_train==False:\n",
    "            return eta\n",
    "        else:\n",
    "            return eta * self.save_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ccac25c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class softmax():\n",
    "    \n",
    "    def calculate_loss(self, x, label):\n",
    "        N, _ = x.shape\n",
    "        self.label = np.zeros_like(x)\n",
    "        for i in range(self.label.shape[0]):\n",
    "            self.label[i, label[i]] = 1\n",
    "\n",
    "        self.x = np.exp(x - np.max(x, axis=1)[:, np.newaxis])   \n",
    "        sum_x = np.sum(self.x, axis=1)[:, np.newaxis]\n",
    "        self.prediction = self.x / sum_x\n",
    "\n",
    "        self.loss = -np.sum(np.log(self.prediction+1e-6) * self.label)  \n",
    "        return self.loss / N\n",
    "\n",
    "    def prediction_func(self, x):\n",
    "        x = np.exp(x - np.max(x, axis=1)[:, np.newaxis])  \n",
    "        sum_x = np.sum(x, axis=1)[:, np.newaxis]\n",
    "        self.out = x / sum_x\n",
    "        return self.out\n",
    "\n",
    "\n",
    "    def gradient(self):\n",
    "        self.eta = self.prediction.copy() - self.label\n",
    "        return self.eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ddea07be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class parameter():\n",
    "    def __init__(self, w):\n",
    "        self.data = w     \n",
    "        self.grad = None  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f2e02476",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "class fc():\n",
    "    def __init__(self, input_num, output_num, bias=True, requires_grad=True):\n",
    "        self.input_num = input_num          \n",
    "        self.output_num = output_num        \n",
    "        self.requires_grad = requires_grad\n",
    "        self.weight = parameter(np.random.randn(self.input_num, self.output_num) * (2/self.input_num**0.5))\n",
    "        if bias:\n",
    "            self.bias = parameter(np.random.randn(self.output_num))\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input_shape = input.shape    \n",
    "        if input.ndim > 2:\n",
    "            N, C, H, W = input.shape\n",
    "            self.x = input.reshape((N, -1))\n",
    "        elif input.ndim == 2:\n",
    "            self.x = input\n",
    "        else:\n",
    "            print(\"fc.forward error\")\n",
    "        result = np.dot(self.x, self.weight.data)\n",
    "        if self.bias is not None:\n",
    "            result = result + self.bias.data\n",
    "        return result\n",
    "\n",
    "\n",
    "    def backward(self, eta, lr):\n",
    "        N, _ = eta.shape\n",
    "        next_eta = np.dot(eta, self.weight.data.T)\n",
    "        self.weight.grad = np.reshape(next_eta, self.input_shape)\n",
    "\n",
    "        \n",
    "        x = self.x.repeat(self.output_num, axis=0).reshape((N, self.output_num, -1))\n",
    "        self.W_grad = x * eta.reshape((N, -1, 1))\n",
    "        self.W_grad = np.sum(self.W_grad, axis=0) / N\n",
    "        self.b_grad = np.sum(eta, axis=0) / N\n",
    "\n",
    "\n",
    "        self.weight.data -= lr * self.W_grad.T\n",
    "        self.bias.data -= lr * self.b_grad\n",
    "\n",
    "        return self.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "61798247",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooling():\n",
    "    def __init__(self, kernel_size=(2, 2), stride=2, ):\n",
    "        self.ksize = kernel_size\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, input):\n",
    "        N, C, H, W = input.shape\n",
    "        out = input.reshape(N, C, H//self.stride, self.stride, W//self.stride, self.stride)\n",
    "        out = out.max(axis=(3,5))\n",
    "        self.mask = out.repeat(self.ksize[0], axis=2).repeat(self.ksize[1], axis=3) != input\n",
    "        return out\n",
    "\n",
    "    def backward(self, eta):\n",
    "        result = eta.repeat(self.ksize[0], axis=2).repeat(self.ksize[1], axis=3)\n",
    "        result[self.mask] = 0\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "314bd687",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BN():\n",
    "    def __init__(self, channel, moving_decay=0.9, is_train=True):\n",
    "        self.alpha = parameter(np.ones((channel,1,1)))\n",
    "        self.beta = parameter(np.zeros((channel,1,1)))\n",
    "        self.is_train = is_train\n",
    "        self.eps = 1e-5 \n",
    "\n",
    "        self.moving_mean = np.zeros((channel,1,1))\n",
    "        self.moving_var = np.zeros((channel,1,1))\n",
    "        self.moving_decay = moving_decay\n",
    "\n",
    "\n",
    "    def forward(self, x, is_train=True):\n",
    "        self.is_train = is_train\n",
    "        N, C, H, W = x.shape\n",
    "        self.x = x\n",
    "\n",
    "        if self.is_train:       \n",
    "            self.mean = np.mean(x, axis=(0,2,3))[:,np.newaxis, np.newaxis]\n",
    "            self.var = np.var(x, axis=(0,2,3))[:,np.newaxis, np.newaxis]\n",
    "\n",
    "            if (np.sum(self.moving_mean)==0) and (np.sum(self.moving_var)==0):\n",
    "                self.moving_mean = self.mean\n",
    "                self.moving_var = self.var\n",
    "            else:\n",
    "                self.moving_mean = self.moving_mean * self.moving_decay + (1-self.moving_decay) * self.mean\n",
    "                self.moving_var = self.moving_var * self.moving_decay + (1-self.moving_decay) * self.var\n",
    "\n",
    "            self.y = (x - self.mean) / np.sqrt(self.var + self.eps)\n",
    "            return  self.alpha.data * self.y + self.beta.data\n",
    "        else:   \n",
    "            self.y = (x - self.moving_mean) / np.sqrt(self.moving_var + self.eps)\n",
    "            return  self.alpha.data * self.y + self.beta.data\n",
    "\n",
    "\n",
    "    def backward(self, eta, lr):\n",
    "        N, _, H, W = eta.shape\n",
    "        alpha_grad = np.sum(eta * self.y, axis=(0,2,3))\n",
    "        beta_grad = np.sum(eta, axis=(0,2,3))\n",
    "\n",
    "        yx_grad = (eta * self.alpha.data)\n",
    "        ymean_grad = (-1.0 / np.sqrt(self.var +self.eps)) * yx_grad\n",
    "        ymean_grad = np.sum(ymean_grad, axis=(2,3))[:,:,np.newaxis,np.newaxis] / (H*W)\n",
    "        yvar_grad = -0.5*yx_grad*(self.x - self.mean) / (self.var+self.eps)**(3.0/2)\n",
    "        yvar_grad = 2 * (self.x-self.mean) * np.sum(yvar_grad,axis=(2,3))[:,:,np.newaxis,np.newaxis] / (H*W)\n",
    "        result = yx_grad*(1 / np.sqrt(self.var +self.eps)) + ymean_grad + yvar_grad\n",
    "\n",
    "\n",
    "        self.alpha.data -= lr * alpha_grad[:,np.newaxis, np.newaxis] / N\n",
    "        self.beta.data -=  lr * beta_grad[:, np.newaxis, np.newaxis] / N\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b98dadcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5():\n",
    "    def __init__(self):\n",
    "        self.conv1 = conv((6, 1, 5, 5), stride=1, padding='SAME', bias=True, requires_grad=True)\n",
    "        self.pooling1 = Pooling(kernel_size=(2, 2), stride=2)\n",
    "        self.BN1 = BN(6, moving_decay=0.9, is_train=True)\n",
    "        self.relu1 = Relu()\n",
    "\n",
    "        self.conv2 = conv((16, 6, 5, 5), stride=1, padding=\"VALID\", bias=True, requires_grad=True)\n",
    "        self.pooling2 = Pooling(kernel_size=(2, 2), stride=2)\n",
    "        self.BN2 = BN(16, moving_decay=0.9, is_train=True)\n",
    "        self.relu2 = Relu()\n",
    "\n",
    "        self.conv3 = conv((120, 16, 5, 5), stride=1, padding=\"VALID\", bias=True, requires_grad=True)\n",
    "\n",
    "        self.fc4 = fc(120*1*1, 84, bias=True, requires_grad=True)\n",
    "        self.relu4 = Relu()\n",
    "        self.fc5 = fc(84, 10, bias=True, requires_grad=True)\n",
    "\n",
    "        self.softmax = softmax()\n",
    "\n",
    "    def forward(self, imgs, labels, is_train=True):\n",
    "        x = self.conv1.forward(imgs)\n",
    "        x = self.pooling1.forward(x)\n",
    "        x = self.BN1.forward(x, is_train)\n",
    "        x = self.relu1.forward(x)\n",
    "\n",
    "        x = self.conv2.forward(x)\n",
    "        x = self.pooling2.forward(x)\n",
    "        x = self.BN2.forward(x, is_train)\n",
    "        x = self.relu2.forward(x)\n",
    "\n",
    "        x = self.conv3.forward(x)\n",
    "\n",
    "        x = self.fc4.forward(x)\n",
    "        x = self.relu4.forward(x)\n",
    "        x = self.fc5.forward(x)\n",
    "\n",
    "        loss = self.softmax.calculate_loss(x, labels)\n",
    "        prediction = self.softmax.prediction_func(x)\n",
    "        return loss, prediction\n",
    "\n",
    "\n",
    "    def backward(self, lr):\n",
    "        eta = self.softmax.gradient()\n",
    "\n",
    "        eta = self.fc5.backward(eta, lr)\n",
    "        eta = self.relu4.backward(eta)\n",
    "        eta = self.fc4.backward(eta, lr)\n",
    "\n",
    "        eta = self.conv3.backward(eta, lr)\n",
    "\n",
    "        eta = self.relu2.backward(eta)  \n",
    "        eta = self.BN2.backward(eta, lr)\n",
    "        eta = self.pooling2.backward(eta)     \n",
    "        eta = self.conv2.backward(eta, lr)\n",
    "\n",
    "        eta = self.relu1.backward(eta)\n",
    "        eta = self.BN1.backward(eta, lr)\n",
    "        eta = self.pooling1.backward(eta)\n",
    "        eta = self.conv1.backward(eta, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "acd0ec5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(x):\n",
    "    eps = 1e-5\n",
    "    if x.ndim > 2:\n",
    "        mean = np.mean(x, axis=(0, 2, 3))[:, np.newaxis, np.newaxis]\n",
    "        var = np.var(x, axis=(0, 2, 3))[:, np.newaxis, np.newaxis]\n",
    "        x = (x - mean) / np.sqrt(var + eps)\n",
    "    else:\n",
    "        mean = np.mean(x, axis=1)[:, np.newaxis]\n",
    "        var = np.var(x, axis=1)[:, np.newaxis] + eps\n",
    "        x = (x - mean) / np.sqrt(var)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e6093da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MNIST data from files...\n",
      "Load images from mnist_data/train-images.idx3-ubyte, number: 60000, data shape: (60000, 784)\n",
      "Load images from mnist_data/train-labels.idx1-ubyte, number: 60000, data shape: (60000, 1)\n",
      "Load images from mnist_data/t10k-images.idx3-ubyte, number: 10000, data shape: (10000, 784)\n",
      "Load images from mnist_data/t10k-labels.idx1-ubyte, number: 10000, data shape: (10000, 1)\n",
      "Got data. \n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import struct\n",
    "import time\n",
    "import os\n",
    "def load_mnist(file_dir, is_images='True'):\n",
    "    bin_file = open(file_dir, 'rb')\n",
    "    bin_data = bin_file.read()\n",
    "    bin_file.close()\n",
    "    if is_images:\n",
    "        fmt_header = '>iiii'\n",
    "        magic, num_images, num_rows, num_cols = struct.unpack_from(fmt_header, bin_data, 0)\n",
    "    else:\n",
    "        fmt_header = '>ii'\n",
    "        magic, num_images = struct.unpack_from(fmt_header, bin_data, 0)\n",
    "        num_rows, num_cols = 1, 1\n",
    "    data_size = num_images * num_rows * num_cols\n",
    "    mat_data = struct.unpack_from('>' + str(data_size) + 'B', bin_data, struct.calcsize(fmt_header))\n",
    "    mat_data = np.reshape(mat_data, [num_images, num_rows * num_cols])\n",
    "    print('Load images from %s, number: %d, data shape: %s' % (file_dir, num_images, str(mat_data.shape)))\n",
    "    return mat_data\n",
    "\n",
    "\n",
    "def data_convert(x, y, m, k):\n",
    "    x[x<=40]=0\n",
    "    x[x>40]=1\n",
    "    ont_hot_y = np.zeros((m,k))    \n",
    "    for t in np.arange(0,m):\n",
    "        ont_hot_y[t,y[t]]=1\n",
    "    ont_hot_y=ont_hot_y.T\n",
    "    return x, ont_hot_y\n",
    "\n",
    "\n",
    "def load_data(mnist_dir, train_data_dir, train_label_dir, test_data_dir, test_label_dir):\n",
    "    print('Loading MNIST data from files...')\n",
    "    train_images = load_mnist(os.path.join(mnist_dir, train_data_dir), True)\n",
    "    train_labels = load_mnist(os.path.join(mnist_dir, train_label_dir), False)\n",
    "    test_images = load_mnist(os.path.join(mnist_dir, test_data_dir), True)\n",
    "    test_labels = load_mnist(os.path.join(mnist_dir, test_label_dir), False)\n",
    "    return train_images, train_labels, test_images, test_labels\n",
    "\n",
    "mnist_dir = \"mnist_data/\"\n",
    "train_data_dir = \"train-images.idx3-ubyte\"\n",
    "train_label_dir = \"train-labels.idx1-ubyte\"\n",
    "test_data_dir = \"t10k-images.idx3-ubyte\"\n",
    "test_label_dir = \"t10k-labels.idx1-ubyte\"\n",
    "\n",
    "train_images, train_labels, test_images, test_labels = load_data(mnist_dir, train_data_dir, train_label_dir, test_data_dir, test_label_dir)\n",
    "print(\"Got data. \") \n",
    "\n",
    "\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0c81ba4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   epoch:    1 , batch:   50 , avg_batch_acc:0.2122 , avg_batch_loss:0.0622 \n",
      "   epoch:    1 , batch:  100 , avg_batch_acc:0.4009 , avg_batch_loss:0.0330 \n",
      "   epoch:    1 , batch:  150 , avg_batch_acc:0.4713 , avg_batch_loss:0.0257 \n",
      "   epoch:    1 , batch:  200 , avg_batch_acc:0.5969 , avg_batch_loss:0.0197 \n",
      "   epoch:    1 , batch:  250 , avg_batch_acc:0.6175 , avg_batch_loss:0.0183 \n",
      "   epoch:    1 , batch:  300 , avg_batch_acc:0.6884 , avg_batch_loss:0.0151 \n",
      "   epoch:    1 , batch:  350 , avg_batch_acc:0.7266 , avg_batch_loss:0.0132 \n",
      "   epoch:    1 , batch:  400 , avg_batch_acc:0.7378 , avg_batch_loss:0.0129 \n",
      "   epoch:    1 , batch:  450 , avg_batch_acc:0.7741 , avg_batch_loss:0.0112 \n",
      "   epoch:    1 , batch:  500 , avg_batch_acc:0.7575 , avg_batch_loss:0.0115 \n",
      "   epoch:    1 , batch:  550 , avg_batch_acc:0.7994 , avg_batch_loss:0.0102 \n",
      "   epoch:    1 , batch:  600 , avg_batch_acc:0.8087 , avg_batch_loss:0.0098 \n",
      "   epoch:    1 , batch:  650 , avg_batch_acc:0.8153 , avg_batch_loss:0.0092 \n",
      "   epoch:    1 , batch:  700 , avg_batch_acc:0.8237 , avg_batch_loss:0.0088 \n",
      "   epoch:    1 , batch:  750 , avg_batch_acc:0.8350 , avg_batch_loss:0.0084 \n",
      "   epoch:    1 , batch:  800 , avg_batch_acc:0.8272 , avg_batch_loss:0.0083 \n",
      "   epoch:    1 , batch:  850 , avg_batch_acc:0.8547 , avg_batch_loss:0.0073 \n",
      "   epoch:    1 , batch:  900 , avg_batch_acc:0.8750 , avg_batch_loss:0.0063 \n",
      "    **********epoch:    1 , avg_epoch_acc:0.7085 , avg_epoch_loss:0.0157 *************\n",
      "------------test_set_acc:0.8648---------------\n",
      "   epoch:    2 , batch:   50 , avg_batch_acc:0.8697 , avg_batch_loss:0.0068 \n",
      "   epoch:    2 , batch:  100 , avg_batch_acc:0.8797 , avg_batch_loss:0.0064 \n",
      "   epoch:    2 , batch:  150 , avg_batch_acc:0.8553 , avg_batch_loss:0.0072 \n",
      "   epoch:    2 , batch:  200 , avg_batch_acc:0.8769 , avg_batch_loss:0.0060 \n",
      "   epoch:    2 , batch:  250 , avg_batch_acc:0.8641 , avg_batch_loss:0.0070 \n",
      "   epoch:    2 , batch:  300 , avg_batch_acc:0.8884 , avg_batch_loss:0.0058 \n",
      "   epoch:    2 , batch:  350 , avg_batch_acc:0.8916 , avg_batch_loss:0.0055 \n",
      "   epoch:    2 , batch:  400 , avg_batch_acc:0.8750 , avg_batch_loss:0.0061 \n",
      "   epoch:    2 , batch:  450 , avg_batch_acc:0.8888 , avg_batch_loss:0.0057 \n",
      "   epoch:    2 , batch:  500 , avg_batch_acc:0.8741 , avg_batch_loss:0.0063 \n",
      "   epoch:    2 , batch:  550 , avg_batch_acc:0.8909 , avg_batch_loss:0.0055 \n",
      "   epoch:    2 , batch:  600 , avg_batch_acc:0.8856 , avg_batch_loss:0.0057 \n",
      "   epoch:    2 , batch:  650 , avg_batch_acc:0.8934 , avg_batch_loss:0.0054 \n",
      "   epoch:    2 , batch:  700 , avg_batch_acc:0.8944 , avg_batch_loss:0.0054 \n",
      "   epoch:    2 , batch:  750 , avg_batch_acc:0.8903 , avg_batch_loss:0.0054 \n",
      "   epoch:    2 , batch:  800 , avg_batch_acc:0.8903 , avg_batch_loss:0.0055 \n",
      "   epoch:    2 , batch:  850 , avg_batch_acc:0.9034 , avg_batch_loss:0.0049 \n",
      "   epoch:    2 , batch:  900 , avg_batch_acc:0.9163 , avg_batch_loss:0.0041 \n",
      "    **********epoch:    2 , avg_epoch_acc:0.8863 , avg_epoch_loss:0.0057 *************\n",
      "------------test_set_acc:0.9074---------------\n",
      "   epoch:    3 , batch:   50 , avg_batch_acc:0.9109 , avg_batch_loss:0.0047 \n",
      "   epoch:    3 , batch:  100 , avg_batch_acc:0.9191 , avg_batch_loss:0.0044 \n",
      "   epoch:    3 , batch:  150 , avg_batch_acc:0.8988 , avg_batch_loss:0.0051 \n",
      "   epoch:    3 , batch:  200 , avg_batch_acc:0.9119 , avg_batch_loss:0.0044 \n",
      "   epoch:    3 , batch:  250 , avg_batch_acc:0.8969 , avg_batch_loss:0.0052 \n",
      "   epoch:    3 , batch:  300 , avg_batch_acc:0.9263 , avg_batch_loss:0.0041 \n",
      "   epoch:    3 , batch:  350 , avg_batch_acc:0.9225 , avg_batch_loss:0.0041 \n",
      "   epoch:    3 , batch:  400 , avg_batch_acc:0.9056 , avg_batch_loss:0.0045 \n",
      "   epoch:    3 , batch:  450 , avg_batch_acc:0.9187 , avg_batch_loss:0.0043 \n",
      "   epoch:    3 , batch:  500 , avg_batch_acc:0.9062 , avg_batch_loss:0.0048 \n",
      "   epoch:    3 , batch:  550 , avg_batch_acc:0.9178 , avg_batch_loss:0.0042 \n",
      "   epoch:    3 , batch:  600 , avg_batch_acc:0.9134 , avg_batch_loss:0.0044 \n",
      "   epoch:    3 , batch:  650 , avg_batch_acc:0.9222 , avg_batch_loss:0.0042 \n",
      "   epoch:    3 , batch:  700 , avg_batch_acc:0.9153 , avg_batch_loss:0.0043 \n",
      "   epoch:    3 , batch:  750 , avg_batch_acc:0.9156 , avg_batch_loss:0.0043 \n",
      "   epoch:    3 , batch:  800 , avg_batch_acc:0.9109 , avg_batch_loss:0.0044 \n",
      "   epoch:    3 , batch:  850 , avg_batch_acc:0.9234 , avg_batch_loss:0.0039 \n",
      "   epoch:    3 , batch:  900 , avg_batch_acc:0.9334 , avg_batch_loss:0.0033 \n",
      "    **********epoch:    3 , avg_epoch_acc:0.9159 , avg_epoch_loss:0.0043 *************\n",
      "------------test_set_acc:0.9261---------------\n",
      "   epoch:    4 , batch:   50 , avg_batch_acc:0.9291 , avg_batch_loss:0.0038 \n",
      "   epoch:    4 , batch:  100 , avg_batch_acc:0.9363 , avg_batch_loss:0.0035 \n",
      "   epoch:    4 , batch:  150 , avg_batch_acc:0.9153 , avg_batch_loss:0.0042 \n",
      "   epoch:    4 , batch:  200 , avg_batch_acc:0.9294 , avg_batch_loss:0.0036 \n",
      "   epoch:    4 , batch:  250 , avg_batch_acc:0.9156 , avg_batch_loss:0.0043 \n",
      "   epoch:    4 , batch:  300 , avg_batch_acc:0.9387 , avg_batch_loss:0.0034 \n",
      "   epoch:    4 , batch:  350 , avg_batch_acc:0.9359 , avg_batch_loss:0.0034 \n",
      "   epoch:    4 , batch:  400 , avg_batch_acc:0.9247 , avg_batch_loss:0.0037 \n",
      "   epoch:    4 , batch:  450 , avg_batch_acc:0.9297 , avg_batch_loss:0.0037 \n",
      "   epoch:    4 , batch:  500 , avg_batch_acc:0.9231 , avg_batch_loss:0.0040 \n",
      "   epoch:    4 , batch:  550 , avg_batch_acc:0.9334 , avg_batch_loss:0.0035 \n",
      "   epoch:    4 , batch:  600 , avg_batch_acc:0.9269 , avg_batch_loss:0.0037 \n",
      "   epoch:    4 , batch:  650 , avg_batch_acc:0.9341 , avg_batch_loss:0.0036 \n",
      "   epoch:    4 , batch:  700 , avg_batch_acc:0.9256 , avg_batch_loss:0.0036 \n",
      "   epoch:    4 , batch:  750 , avg_batch_acc:0.9278 , avg_batch_loss:0.0036 \n",
      "   epoch:    4 , batch:  800 , avg_batch_acc:0.9266 , avg_batch_loss:0.0038 \n",
      "   epoch:    4 , batch:  850 , avg_batch_acc:0.9353 , avg_batch_loss:0.0034 \n",
      "   epoch:    4 , batch:  900 , avg_batch_acc:0.9437 , avg_batch_loss:0.0028 \n",
      "    **********epoch:    4 , avg_epoch_acc:0.9302 , avg_epoch_loss:0.0036 *************\n",
      "------------test_set_acc:0.9358---------------\n",
      "   epoch:    5 , batch:   50 , avg_batch_acc:0.9403 , avg_batch_loss:0.0032 \n",
      "   epoch:    5 , batch:  100 , avg_batch_acc:0.9434 , avg_batch_loss:0.0030 \n",
      "   epoch:    5 , batch:  150 , avg_batch_acc:0.9281 , avg_batch_loss:0.0036 \n",
      "   epoch:    5 , batch:  200 , avg_batch_acc:0.9384 , avg_batch_loss:0.0032 \n",
      "   epoch:    5 , batch:  250 , avg_batch_acc:0.9259 , avg_batch_loss:0.0037 \n",
      "   epoch:    5 , batch:  300 , avg_batch_acc:0.9450 , avg_batch_loss:0.0029 \n",
      "   epoch:    5 , batch:  350 , avg_batch_acc:0.9416 , avg_batch_loss:0.0030 \n",
      "   epoch:    5 , batch:  400 , avg_batch_acc:0.9334 , avg_batch_loss:0.0031 \n",
      "   epoch:    5 , batch:  450 , avg_batch_acc:0.9353 , avg_batch_loss:0.0032 \n",
      "   epoch:    5 , batch:  500 , avg_batch_acc:0.9337 , avg_batch_loss:0.0034 \n",
      "   epoch:    5 , batch:  550 , avg_batch_acc:0.9422 , avg_batch_loss:0.0031 \n",
      "   epoch:    5 , batch:  600 , avg_batch_acc:0.9381 , avg_batch_loss:0.0033 \n",
      "   epoch:    5 , batch:  650 , avg_batch_acc:0.9419 , avg_batch_loss:0.0032 \n",
      "   epoch:    5 , batch:  700 , avg_batch_acc:0.9350 , avg_batch_loss:0.0032 \n",
      "   epoch:    5 , batch:  750 , avg_batch_acc:0.9331 , avg_batch_loss:0.0032 \n",
      "   epoch:    5 , batch:  800 , avg_batch_acc:0.9337 , avg_batch_loss:0.0034 \n",
      "   epoch:    5 , batch:  850 , avg_batch_acc:0.9416 , avg_batch_loss:0.0030 \n",
      "   epoch:    5 , batch:  900 , avg_batch_acc:0.9500 , avg_batch_loss:0.0025 \n",
      "    **********epoch:    5 , avg_epoch_acc:0.9383 , avg_epoch_loss:0.0031 *************\n",
      "------------test_set_acc:0.9423---------------\n",
      "   epoch:    6 , batch:   50 , avg_batch_acc:0.9484 , avg_batch_loss:0.0029 \n",
      "   epoch:    6 , batch:  100 , avg_batch_acc:0.9481 , avg_batch_loss:0.0027 \n",
      "   epoch:    6 , batch:  150 , avg_batch_acc:0.9369 , avg_batch_loss:0.0032 \n",
      "   epoch:    6 , batch:  200 , avg_batch_acc:0.9475 , avg_batch_loss:0.0028 \n",
      "   epoch:    6 , batch:  250 , avg_batch_acc:0.9334 , avg_batch_loss:0.0033 \n",
      "   epoch:    6 , batch:  300 , avg_batch_acc:0.9497 , avg_batch_loss:0.0026 \n",
      "   epoch:    6 , batch:  350 , avg_batch_acc:0.9444 , avg_batch_loss:0.0027 \n",
      "   epoch:    6 , batch:  400 , avg_batch_acc:0.9419 , avg_batch_loss:0.0028 \n",
      "   epoch:    6 , batch:  450 , avg_batch_acc:0.9406 , avg_batch_loss:0.0030 \n",
      "   epoch:    6 , batch:  500 , avg_batch_acc:0.9416 , avg_batch_loss:0.0031 \n",
      "   epoch:    6 , batch:  550 , avg_batch_acc:0.9478 , avg_batch_loss:0.0027 \n",
      "   epoch:    6 , batch:  600 , avg_batch_acc:0.9447 , avg_batch_loss:0.0029 \n",
      "   epoch:    6 , batch:  650 , avg_batch_acc:0.9463 , avg_batch_loss:0.0029 \n",
      "   epoch:    6 , batch:  700 , avg_batch_acc:0.9419 , avg_batch_loss:0.0029 \n",
      "   epoch:    6 , batch:  750 , avg_batch_acc:0.9391 , avg_batch_loss:0.0029 \n",
      "   epoch:    6 , batch:  800 , avg_batch_acc:0.9400 , avg_batch_loss:0.0031 \n",
      "   epoch:    6 , batch:  850 , avg_batch_acc:0.9447 , avg_batch_loss:0.0028 \n",
      "   epoch:    6 , batch:  900 , avg_batch_acc:0.9544 , avg_batch_loss:0.0023 \n",
      "    **********epoch:    6 , avg_epoch_acc:0.9443 , avg_epoch_loss:0.0028 *************\n",
      "------------test_set_acc:0.9475---------------\n",
      "   epoch:    7 , batch:   50 , avg_batch_acc:0.9528 , avg_batch_loss:0.0026 \n",
      "   epoch:    7 , batch:  100 , avg_batch_acc:0.9516 , avg_batch_loss:0.0025 \n",
      "   epoch:    7 , batch:  150 , avg_batch_acc:0.9413 , avg_batch_loss:0.0029 \n",
      "   epoch:    7 , batch:  200 , avg_batch_acc:0.9519 , avg_batch_loss:0.0026 \n",
      "   epoch:    7 , batch:  250 , avg_batch_acc:0.9381 , avg_batch_loss:0.0030 \n",
      "   epoch:    7 , batch:  300 , avg_batch_acc:0.9556 , avg_batch_loss:0.0024 \n",
      "   epoch:    7 , batch:  350 , avg_batch_acc:0.9494 , avg_batch_loss:0.0024 \n",
      "   epoch:    7 , batch:  400 , avg_batch_acc:0.9484 , avg_batch_loss:0.0025 \n",
      "   epoch:    7 , batch:  450 , avg_batch_acc:0.9428 , avg_batch_loss:0.0027 \n",
      "   epoch:    7 , batch:  500 , avg_batch_acc:0.9472 , avg_batch_loss:0.0028 \n",
      "   epoch:    7 , batch:  550 , avg_batch_acc:0.9534 , avg_batch_loss:0.0025 \n",
      "   epoch:    7 , batch:  600 , avg_batch_acc:0.9484 , avg_batch_loss:0.0027 \n",
      "   epoch:    7 , batch:  650 , avg_batch_acc:0.9516 , avg_batch_loss:0.0026 \n",
      "   epoch:    7 , batch:  700 , avg_batch_acc:0.9456 , avg_batch_loss:0.0027 \n",
      "   epoch:    7 , batch:  750 , avg_batch_acc:0.9425 , avg_batch_loss:0.0027 \n",
      "   epoch:    7 , batch:  800 , avg_batch_acc:0.9441 , avg_batch_loss:0.0028 \n",
      "   epoch:    7 , batch:  850 , avg_batch_acc:0.9481 , avg_batch_loss:0.0026 \n",
      "   epoch:    7 , batch:  900 , avg_batch_acc:0.9566 , avg_batch_loss:0.0021 \n",
      "    **********epoch:    7 , avg_epoch_acc:0.9486 , avg_epoch_loss:0.0026 *************\n",
      "------------test_set_acc:0.9513---------------\n",
      "   epoch:    8 , batch:   50 , avg_batch_acc:0.9541 , avg_batch_loss:0.0024 \n",
      "   epoch:    8 , batch:  100 , avg_batch_acc:0.9544 , avg_batch_loss:0.0023 \n",
      "   epoch:    8 , batch:  150 , avg_batch_acc:0.9466 , avg_batch_loss:0.0027 \n",
      "   epoch:    8 , batch:  200 , avg_batch_acc:0.9563 , avg_batch_loss:0.0024 \n",
      "   epoch:    8 , batch:  250 , avg_batch_acc:0.9431 , avg_batch_loss:0.0028 \n",
      "   epoch:    8 , batch:  300 , avg_batch_acc:0.9603 , avg_batch_loss:0.0022 \n",
      "   epoch:    8 , batch:  350 , avg_batch_acc:0.9516 , avg_batch_loss:0.0023 \n",
      "   epoch:    8 , batch:  400 , avg_batch_acc:0.9537 , avg_batch_loss:0.0023 \n",
      "   epoch:    8 , batch:  450 , avg_batch_acc:0.9481 , avg_batch_loss:0.0026 \n",
      "   epoch:    8 , batch:  500 , avg_batch_acc:0.9484 , avg_batch_loss:0.0026 \n",
      "   epoch:    8 , batch:  550 , avg_batch_acc:0.9569 , avg_batch_loss:0.0023 \n",
      "   epoch:    8 , batch:  600 , avg_batch_acc:0.9513 , avg_batch_loss:0.0025 \n",
      "   epoch:    8 , batch:  650 , avg_batch_acc:0.9544 , avg_batch_loss:0.0025 \n",
      "   epoch:    8 , batch:  700 , avg_batch_acc:0.9497 , avg_batch_loss:0.0025 \n",
      "   epoch:    8 , batch:  750 , avg_batch_acc:0.9472 , avg_batch_loss:0.0025 \n",
      "   epoch:    8 , batch:  800 , avg_batch_acc:0.9484 , avg_batch_loss:0.0026 \n",
      "   epoch:    8 , batch:  850 , avg_batch_acc:0.9509 , avg_batch_loss:0.0024 \n",
      "   epoch:    8 , batch:  900 , avg_batch_acc:0.9616 , avg_batch_loss:0.0020 \n",
      "    **********epoch:    8 , avg_epoch_acc:0.9523 , avg_epoch_loss:0.0024 *************\n",
      "------------test_set_acc:0.9549---------------\n",
      "   epoch:    9 , batch:   50 , avg_batch_acc:0.9578 , avg_batch_loss:0.0022 \n",
      "   epoch:    9 , batch:  100 , avg_batch_acc:0.9575 , avg_batch_loss:0.0021 \n",
      "   epoch:    9 , batch:  150 , avg_batch_acc:0.9503 , avg_batch_loss:0.0025 \n",
      "   epoch:    9 , batch:  200 , avg_batch_acc:0.9587 , avg_batch_loss:0.0023 \n",
      "   epoch:    9 , batch:  250 , avg_batch_acc:0.9484 , avg_batch_loss:0.0026 \n",
      "   epoch:    9 , batch:  300 , avg_batch_acc:0.9631 , avg_batch_loss:0.0020 \n",
      "   epoch:    9 , batch:  350 , avg_batch_acc:0.9563 , avg_batch_loss:0.0021 \n",
      "   epoch:    9 , batch:  400 , avg_batch_acc:0.9563 , avg_batch_loss:0.0021 \n",
      "   epoch:    9 , batch:  450 , avg_batch_acc:0.9506 , avg_batch_loss:0.0024 \n",
      "   epoch:    9 , batch:  500 , avg_batch_acc:0.9528 , avg_batch_loss:0.0024 \n",
      "   epoch:    9 , batch:  550 , avg_batch_acc:0.9609 , avg_batch_loss:0.0022 \n",
      "   epoch:    9 , batch:  600 , avg_batch_acc:0.9559 , avg_batch_loss:0.0024 \n",
      "   epoch:    9 , batch:  650 , avg_batch_acc:0.9587 , avg_batch_loss:0.0023 \n",
      "   epoch:    9 , batch:  700 , avg_batch_acc:0.9537 , avg_batch_loss:0.0024 \n",
      "   epoch:    9 , batch:  750 , avg_batch_acc:0.9509 , avg_batch_loss:0.0024 \n",
      "   epoch:    9 , batch:  800 , avg_batch_acc:0.9522 , avg_batch_loss:0.0025 \n",
      "   epoch:    9 , batch:  850 , avg_batch_acc:0.9541 , avg_batch_loss:0.0023 \n",
      "   epoch:    9 , batch:  900 , avg_batch_acc:0.9625 , avg_batch_loss:0.0019 \n",
      "    **********epoch:    9 , avg_epoch_acc:0.9558 , avg_epoch_loss:0.0022 *************\n",
      "------------test_set_acc:0.9577---------------\n",
      "   epoch:   10 , batch:   50 , avg_batch_acc:0.9622 , avg_batch_loss:0.0021 \n",
      "   epoch:   10 , batch:  100 , avg_batch_acc:0.9600 , avg_batch_loss:0.0020 \n",
      "   epoch:   10 , batch:  150 , avg_batch_acc:0.9534 , avg_batch_loss:0.0023 \n",
      "   epoch:   10 , batch:  200 , avg_batch_acc:0.9603 , avg_batch_loss:0.0021 \n",
      "   epoch:   10 , batch:  250 , avg_batch_acc:0.9516 , avg_batch_loss:0.0024 \n",
      "   epoch:   10 , batch:  300 , avg_batch_acc:0.9663 , avg_batch_loss:0.0019 \n",
      "   epoch:   10 , batch:  350 , avg_batch_acc:0.9603 , avg_batch_loss:0.0020 \n",
      "   epoch:   10 , batch:  400 , avg_batch_acc:0.9597 , avg_batch_loss:0.0020 \n",
      "   epoch:   10 , batch:  450 , avg_batch_acc:0.9544 , avg_batch_loss:0.0023 \n",
      "   epoch:   10 , batch:  500 , avg_batch_acc:0.9553 , avg_batch_loss:0.0022 \n",
      "   epoch:   10 , batch:  550 , avg_batch_acc:0.9628 , avg_batch_loss:0.0021 \n",
      "   epoch:   10 , batch:  600 , avg_batch_acc:0.9594 , avg_batch_loss:0.0022 \n",
      "   epoch:   10 , batch:  650 , avg_batch_acc:0.9591 , avg_batch_loss:0.0022 \n",
      "   epoch:   10 , batch:  700 , avg_batch_acc:0.9575 , avg_batch_loss:0.0022 \n",
      "   epoch:   10 , batch:  750 , avg_batch_acc:0.9525 , avg_batch_loss:0.0023 \n",
      "   epoch:   10 , batch:  800 , avg_batch_acc:0.9537 , avg_batch_loss:0.0024 \n",
      "   epoch:   10 , batch:  850 , avg_batch_acc:0.9572 , avg_batch_loss:0.0021 \n",
      "   epoch:   10 , batch:  900 , avg_batch_acc:0.9641 , avg_batch_loss:0.0017 \n",
      "    **********epoch:   10 , avg_epoch_acc:0.9585 , avg_epoch_loss:0.0021 *************\n",
      "------------test_set_acc:0.9603---------------\n",
      "   epoch:   11 , batch:   50 , avg_batch_acc:0.9634 , avg_batch_loss:0.0020 \n",
      "   epoch:   11 , batch:  100 , avg_batch_acc:0.9619 , avg_batch_loss:0.0019 \n",
      "   epoch:   11 , batch:  150 , avg_batch_acc:0.9556 , avg_batch_loss:0.0022 \n",
      "   epoch:   11 , batch:  200 , avg_batch_acc:0.9628 , avg_batch_loss:0.0020 \n",
      "   epoch:   11 , batch:  250 , avg_batch_acc:0.9550 , avg_batch_loss:0.0023 \n",
      "   epoch:   11 , batch:  300 , avg_batch_acc:0.9688 , avg_batch_loss:0.0018 \n",
      "   epoch:   11 , batch:  350 , avg_batch_acc:0.9634 , avg_batch_loss:0.0019 \n",
      "   epoch:   11 , batch:  400 , avg_batch_acc:0.9625 , avg_batch_loss:0.0018 \n",
      "   epoch:   11 , batch:  450 , avg_batch_acc:0.9563 , avg_batch_loss:0.0022 \n",
      "   epoch:   11 , batch:  500 , avg_batch_acc:0.9578 , avg_batch_loss:0.0021 \n",
      "   epoch:   11 , batch:  550 , avg_batch_acc:0.9650 , avg_batch_loss:0.0020 \n",
      "   epoch:   11 , batch:  600 , avg_batch_acc:0.9609 , avg_batch_loss:0.0021 \n",
      "   epoch:   11 , batch:  650 , avg_batch_acc:0.9606 , avg_batch_loss:0.0021 \n",
      "   epoch:   11 , batch:  700 , avg_batch_acc:0.9600 , avg_batch_loss:0.0021 \n",
      "   epoch:   11 , batch:  750 , avg_batch_acc:0.9534 , avg_batch_loss:0.0022 \n",
      "   epoch:   11 , batch:  800 , avg_batch_acc:0.9566 , avg_batch_loss:0.0022 \n",
      "   epoch:   11 , batch:  850 , avg_batch_acc:0.9597 , avg_batch_loss:0.0020 \n",
      "   epoch:   11 , batch:  900 , avg_batch_acc:0.9681 , avg_batch_loss:0.0017 \n",
      "    **********epoch:   11 , avg_epoch_acc:0.9608 , avg_epoch_loss:0.0020 *************\n",
      "------------test_set_acc:0.9620---------------\n",
      "   epoch:   12 , batch:   50 , avg_batch_acc:0.9644 , avg_batch_loss:0.0019 \n",
      "   epoch:   12 , batch:  100 , avg_batch_acc:0.9647 , avg_batch_loss:0.0018 \n",
      "   epoch:   12 , batch:  150 , avg_batch_acc:0.9578 , avg_batch_loss:0.0021 \n",
      "   epoch:   12 , batch:  200 , avg_batch_acc:0.9653 , avg_batch_loss:0.0019 \n",
      "   epoch:   12 , batch:  250 , avg_batch_acc:0.9581 , avg_batch_loss:0.0022 \n",
      "   epoch:   12 , batch:  300 , avg_batch_acc:0.9706 , avg_batch_loss:0.0017 \n",
      "   epoch:   12 , batch:  350 , avg_batch_acc:0.9656 , avg_batch_loss:0.0018 \n",
      "   epoch:   12 , batch:  400 , avg_batch_acc:0.9663 , avg_batch_loss:0.0017 \n",
      "   epoch:   12 , batch:  450 , avg_batch_acc:0.9578 , avg_batch_loss:0.0021 \n",
      "   epoch:   12 , batch:  500 , avg_batch_acc:0.9600 , avg_batch_loss:0.0020 \n",
      "   epoch:   12 , batch:  550 , avg_batch_acc:0.9663 , avg_batch_loss:0.0019 \n",
      "   epoch:   12 , batch:  600 , avg_batch_acc:0.9619 , avg_batch_loss:0.0020 \n",
      "   epoch:   12 , batch:  650 , avg_batch_acc:0.9625 , avg_batch_loss:0.0020 \n",
      "   epoch:   12 , batch:  700 , avg_batch_acc:0.9631 , avg_batch_loss:0.0020 \n",
      "   epoch:   12 , batch:  750 , avg_batch_acc:0.9553 , avg_batch_loss:0.0021 \n",
      "   epoch:   12 , batch:  800 , avg_batch_acc:0.9581 , avg_batch_loss:0.0021 \n",
      "   epoch:   12 , batch:  850 , avg_batch_acc:0.9606 , avg_batch_loss:0.0019 \n",
      "   epoch:   12 , batch:  900 , avg_batch_acc:0.9703 , avg_batch_loss:0.0016 \n",
      "    **********epoch:   12 , avg_epoch_acc:0.9628 , avg_epoch_loss:0.0019 *************\n",
      "------------test_set_acc:0.9633---------------\n",
      "   epoch:   13 , batch:   50 , avg_batch_acc:0.9675 , avg_batch_loss:0.0018 \n",
      "   epoch:   13 , batch:  100 , avg_batch_acc:0.9656 , avg_batch_loss:0.0017 \n",
      "   epoch:   13 , batch:  150 , avg_batch_acc:0.9606 , avg_batch_loss:0.0020 \n",
      "   epoch:   13 , batch:  200 , avg_batch_acc:0.9656 , avg_batch_loss:0.0018 \n",
      "   epoch:   13 , batch:  250 , avg_batch_acc:0.9594 , avg_batch_loss:0.0021 \n",
      "   epoch:   13 , batch:  300 , avg_batch_acc:0.9712 , avg_batch_loss:0.0016 \n",
      "   epoch:   13 , batch:  350 , avg_batch_acc:0.9666 , avg_batch_loss:0.0017 \n",
      "   epoch:   13 , batch:  400 , avg_batch_acc:0.9681 , avg_batch_loss:0.0016 \n",
      "   epoch:   13 , batch:  450 , avg_batch_acc:0.9584 , avg_batch_loss:0.0020 \n",
      "   epoch:   13 , batch:  500 , avg_batch_acc:0.9619 , avg_batch_loss:0.0019 \n",
      "   epoch:   13 , batch:  550 , avg_batch_acc:0.9678 , avg_batch_loss:0.0018 \n",
      "   epoch:   13 , batch:  600 , avg_batch_acc:0.9631 , avg_batch_loss:0.0019 \n",
      "   epoch:   13 , batch:  650 , avg_batch_acc:0.9650 , avg_batch_loss:0.0019 \n",
      "   epoch:   13 , batch:  700 , avg_batch_acc:0.9653 , avg_batch_loss:0.0019 \n",
      "   epoch:   13 , batch:  750 , avg_batch_acc:0.9569 , avg_batch_loss:0.0020 \n",
      "   epoch:   13 , batch:  800 , avg_batch_acc:0.9600 , avg_batch_loss:0.0020 \n",
      "   epoch:   13 , batch:  850 , avg_batch_acc:0.9619 , avg_batch_loss:0.0018 \n",
      "   epoch:   13 , batch:  900 , avg_batch_acc:0.9725 , avg_batch_loss:0.0015 \n",
      "    **********epoch:   13 , avg_epoch_acc:0.9644 , avg_epoch_loss:0.0018 *************\n",
      "------------test_set_acc:0.9644---------------\n",
      "   epoch:   14 , batch:   50 , avg_batch_acc:0.9684 , avg_batch_loss:0.0017 \n",
      "   epoch:   14 , batch:  100 , avg_batch_acc:0.9684 , avg_batch_loss:0.0016 \n",
      "   epoch:   14 , batch:  150 , avg_batch_acc:0.9625 , avg_batch_loss:0.0019 \n",
      "   epoch:   14 , batch:  200 , avg_batch_acc:0.9666 , avg_batch_loss:0.0017 \n",
      "   epoch:   14 , batch:  250 , avg_batch_acc:0.9609 , avg_batch_loss:0.0020 \n",
      "   epoch:   14 , batch:  300 , avg_batch_acc:0.9722 , avg_batch_loss:0.0016 \n",
      "   epoch:   14 , batch:  350 , avg_batch_acc:0.9684 , avg_batch_loss:0.0016 \n",
      "   epoch:   14 , batch:  400 , avg_batch_acc:0.9697 , avg_batch_loss:0.0016 \n",
      "   epoch:   14 , batch:  450 , avg_batch_acc:0.9613 , avg_batch_loss:0.0019 \n",
      "   epoch:   14 , batch:  500 , avg_batch_acc:0.9637 , avg_batch_loss:0.0018 \n",
      "   epoch:   14 , batch:  550 , avg_batch_acc:0.9694 , avg_batch_loss:0.0017 \n",
      "   epoch:   14 , batch:  600 , avg_batch_acc:0.9634 , avg_batch_loss:0.0019 \n",
      "   epoch:   14 , batch:  650 , avg_batch_acc:0.9656 , avg_batch_loss:0.0018 \n",
      "   epoch:   14 , batch:  700 , avg_batch_acc:0.9684 , avg_batch_loss:0.0019 \n",
      "   epoch:   14 , batch:  750 , avg_batch_acc:0.9566 , avg_batch_loss:0.0020 \n",
      "   epoch:   14 , batch:  800 , avg_batch_acc:0.9628 , avg_batch_loss:0.0020 \n",
      "   epoch:   14 , batch:  850 , avg_batch_acc:0.9634 , avg_batch_loss:0.0018 \n",
      "   epoch:   14 , batch:  900 , avg_batch_acc:0.9734 , avg_batch_loss:0.0014 \n",
      "    **********epoch:   14 , avg_epoch_acc:0.9659 , avg_epoch_loss:0.0018 *************\n",
      "------------test_set_acc:0.9667---------------\n",
      "   epoch:   15 , batch:   50 , avg_batch_acc:0.9700 , avg_batch_loss:0.0017 \n",
      "   epoch:   15 , batch:  100 , avg_batch_acc:0.9697 , avg_batch_loss:0.0016 \n",
      "   epoch:   15 , batch:  150 , avg_batch_acc:0.9634 , avg_batch_loss:0.0019 \n",
      "   epoch:   15 , batch:  200 , avg_batch_acc:0.9678 , avg_batch_loss:0.0017 \n",
      "   epoch:   15 , batch:  250 , avg_batch_acc:0.9619 , avg_batch_loss:0.0019 \n",
      "   epoch:   15 , batch:  300 , avg_batch_acc:0.9731 , avg_batch_loss:0.0015 \n",
      "   epoch:   15 , batch:  350 , avg_batch_acc:0.9697 , avg_batch_loss:0.0016 \n",
      "   epoch:   15 , batch:  400 , avg_batch_acc:0.9719 , avg_batch_loss:0.0015 \n",
      "   epoch:   15 , batch:  450 , avg_batch_acc:0.9631 , avg_batch_loss:0.0019 \n",
      "   epoch:   15 , batch:  500 , avg_batch_acc:0.9653 , avg_batch_loss:0.0017 \n",
      "   epoch:   15 , batch:  550 , avg_batch_acc:0.9697 , avg_batch_loss:0.0017 \n",
      "   epoch:   15 , batch:  600 , avg_batch_acc:0.9656 , avg_batch_loss:0.0018 \n",
      "   epoch:   15 , batch:  650 , avg_batch_acc:0.9663 , avg_batch_loss:0.0018 \n",
      "   epoch:   15 , batch:  700 , avg_batch_acc:0.9700 , avg_batch_loss:0.0018 \n",
      "   epoch:   15 , batch:  750 , avg_batch_acc:0.9572 , avg_batch_loss:0.0019 \n",
      "   epoch:   15 , batch:  800 , avg_batch_acc:0.9634 , avg_batch_loss:0.0019 \n",
      "   epoch:   15 , batch:  850 , avg_batch_acc:0.9631 , avg_batch_loss:0.0017 \n",
      "   epoch:   15 , batch:  900 , avg_batch_acc:0.9744 , avg_batch_loss:0.0014 \n",
      "    **********epoch:   15 , avg_epoch_acc:0.9670 , avg_epoch_loss:0.0017 *************\n",
      "------------test_set_acc:0.9677---------------\n",
      "   epoch:   16 , batch:   50 , avg_batch_acc:0.9706 , avg_batch_loss:0.0016 \n",
      "   epoch:   16 , batch:  100 , avg_batch_acc:0.9719 , avg_batch_loss:0.0015 \n",
      "   epoch:   16 , batch:  150 , avg_batch_acc:0.9634 , avg_batch_loss:0.0018 \n",
      "   epoch:   16 , batch:  200 , avg_batch_acc:0.9688 , avg_batch_loss:0.0016 \n",
      "   epoch:   16 , batch:  250 , avg_batch_acc:0.9625 , avg_batch_loss:0.0018 \n",
      "   epoch:   16 , batch:  300 , avg_batch_acc:0.9738 , avg_batch_loss:0.0015 \n",
      "   epoch:   16 , batch:  350 , avg_batch_acc:0.9706 , avg_batch_loss:0.0015 \n",
      "   epoch:   16 , batch:  400 , avg_batch_acc:0.9734 , avg_batch_loss:0.0014 \n",
      "   epoch:   16 , batch:  450 , avg_batch_acc:0.9634 , avg_batch_loss:0.0018 \n",
      "   epoch:   16 , batch:  500 , avg_batch_acc:0.9666 , avg_batch_loss:0.0017 \n",
      "   epoch:   16 , batch:  550 , avg_batch_acc:0.9712 , avg_batch_loss:0.0016 \n",
      "   epoch:   16 , batch:  600 , avg_batch_acc:0.9663 , avg_batch_loss:0.0017 \n",
      "   epoch:   16 , batch:  650 , avg_batch_acc:0.9675 , avg_batch_loss:0.0017 \n",
      "   epoch:   16 , batch:  700 , avg_batch_acc:0.9709 , avg_batch_loss:0.0017 \n",
      "   epoch:   16 , batch:  750 , avg_batch_acc:0.9597 , avg_batch_loss:0.0018 \n",
      "   epoch:   16 , batch:  800 , avg_batch_acc:0.9641 , avg_batch_loss:0.0018 \n",
      "   epoch:   16 , batch:  850 , avg_batch_acc:0.9644 , avg_batch_loss:0.0016 \n",
      "   epoch:   16 , batch:  900 , avg_batch_acc:0.9744 , avg_batch_loss:0.0013 \n",
      "    **********epoch:   16 , avg_epoch_acc:0.9680 , avg_epoch_loss:0.0016 *************\n",
      "------------test_set_acc:0.9688---------------\n",
      "   epoch:   17 , batch:   50 , avg_batch_acc:0.9716 , avg_batch_loss:0.0016 \n",
      "   epoch:   17 , batch:  100 , avg_batch_acc:0.9722 , avg_batch_loss:0.0015 \n",
      "   epoch:   17 , batch:  150 , avg_batch_acc:0.9650 , avg_batch_loss:0.0017 \n",
      "   epoch:   17 , batch:  200 , avg_batch_acc:0.9697 , avg_batch_loss:0.0015 \n",
      "   epoch:   17 , batch:  250 , avg_batch_acc:0.9634 , avg_batch_loss:0.0018 \n",
      "   epoch:   17 , batch:  300 , avg_batch_acc:0.9741 , avg_batch_loss:0.0014 \n",
      "   epoch:   17 , batch:  350 , avg_batch_acc:0.9716 , avg_batch_loss:0.0015 \n",
      "   epoch:   17 , batch:  400 , avg_batch_acc:0.9741 , avg_batch_loss:0.0014 \n",
      "   epoch:   17 , batch:  450 , avg_batch_acc:0.9650 , avg_batch_loss:0.0018 \n",
      "   epoch:   17 , batch:  500 , avg_batch_acc:0.9681 , avg_batch_loss:0.0016 \n",
      "   epoch:   17 , batch:  550 , avg_batch_acc:0.9712 , avg_batch_loss:0.0016 \n",
      "   epoch:   17 , batch:  600 , avg_batch_acc:0.9672 , avg_batch_loss:0.0017 \n",
      "   epoch:   17 , batch:  650 , avg_batch_acc:0.9681 , avg_batch_loss:0.0016 \n",
      "   epoch:   17 , batch:  700 , avg_batch_acc:0.9716 , avg_batch_loss:0.0017 \n",
      "   epoch:   17 , batch:  750 , avg_batch_acc:0.9600 , avg_batch_loss:0.0018 \n",
      "   epoch:   17 , batch:  800 , avg_batch_acc:0.9656 , avg_batch_loss:0.0018 \n",
      "   epoch:   17 , batch:  850 , avg_batch_acc:0.9663 , avg_batch_loss:0.0016 \n",
      "   epoch:   17 , batch:  900 , avg_batch_acc:0.9762 , avg_batch_loss:0.0013 \n",
      "    **********epoch:   17 , avg_epoch_acc:0.9689 , avg_epoch_loss:0.0016 *************\n",
      "------------test_set_acc:0.9698---------------\n",
      "   epoch:   18 , batch:   50 , avg_batch_acc:0.9722 , avg_batch_loss:0.0015 \n",
      "   epoch:   18 , batch:  100 , avg_batch_acc:0.9741 , avg_batch_loss:0.0014 \n",
      "   epoch:   18 , batch:  150 , avg_batch_acc:0.9653 , avg_batch_loss:0.0017 \n",
      "   epoch:   18 , batch:  200 , avg_batch_acc:0.9709 , avg_batch_loss:0.0015 \n",
      "   epoch:   18 , batch:  250 , avg_batch_acc:0.9650 , avg_batch_loss:0.0017 \n",
      "   epoch:   18 , batch:  300 , avg_batch_acc:0.9753 , avg_batch_loss:0.0014 \n",
      "   epoch:   18 , batch:  350 , avg_batch_acc:0.9728 , avg_batch_loss:0.0014 \n",
      "   epoch:   18 , batch:  400 , avg_batch_acc:0.9750 , avg_batch_loss:0.0013 \n",
      "   epoch:   18 , batch:  450 , avg_batch_acc:0.9672 , avg_batch_loss:0.0017 \n",
      "   epoch:   18 , batch:  500 , avg_batch_acc:0.9691 , avg_batch_loss:0.0015 \n",
      "   epoch:   18 , batch:  550 , avg_batch_acc:0.9738 , avg_batch_loss:0.0015 \n",
      "   epoch:   18 , batch:  600 , avg_batch_acc:0.9678 , avg_batch_loss:0.0016 \n",
      "   epoch:   18 , batch:  650 , avg_batch_acc:0.9697 , avg_batch_loss:0.0016 \n",
      "   epoch:   18 , batch:  700 , avg_batch_acc:0.9722 , avg_batch_loss:0.0016 \n",
      "   epoch:   18 , batch:  750 , avg_batch_acc:0.9603 , avg_batch_loss:0.0018 \n",
      "   epoch:   18 , batch:  800 , avg_batch_acc:0.9659 , avg_batch_loss:0.0017 \n",
      "   epoch:   18 , batch:  850 , avg_batch_acc:0.9672 , avg_batch_loss:0.0015 \n",
      "   epoch:   18 , batch:  900 , avg_batch_acc:0.9772 , avg_batch_loss:0.0013 \n",
      "    **********epoch:   18 , avg_epoch_acc:0.9700 , avg_epoch_loss:0.0015 *************\n",
      "------------test_set_acc:0.9705---------------\n",
      "   epoch:   19 , batch:   50 , avg_batch_acc:0.9725 , avg_batch_loss:0.0015 \n",
      "   epoch:   19 , batch:  100 , avg_batch_acc:0.9741 , avg_batch_loss:0.0014 \n",
      "   epoch:   19 , batch:  150 , avg_batch_acc:0.9659 , avg_batch_loss:0.0016 \n",
      "   epoch:   19 , batch:  200 , avg_batch_acc:0.9725 , avg_batch_loss:0.0014 \n",
      "   epoch:   19 , batch:  250 , avg_batch_acc:0.9663 , avg_batch_loss:0.0016 \n",
      "   epoch:   19 , batch:  300 , avg_batch_acc:0.9766 , avg_batch_loss:0.0013 \n",
      "   epoch:   19 , batch:  350 , avg_batch_acc:0.9738 , avg_batch_loss:0.0014 \n",
      "   epoch:   19 , batch:  400 , avg_batch_acc:0.9750 , avg_batch_loss:0.0013 \n",
      "   epoch:   19 , batch:  450 , avg_batch_acc:0.9694 , avg_batch_loss:0.0017 \n",
      "   epoch:   19 , batch:  500 , avg_batch_acc:0.9709 , avg_batch_loss:0.0015 \n",
      "   epoch:   19 , batch:  550 , avg_batch_acc:0.9747 , avg_batch_loss:0.0015 \n",
      "   epoch:   19 , batch:  600 , avg_batch_acc:0.9688 , avg_batch_loss:0.0016 \n",
      "   epoch:   19 , batch:  650 , avg_batch_acc:0.9706 , avg_batch_loss:0.0015 \n",
      "   epoch:   19 , batch:  700 , avg_batch_acc:0.9734 , avg_batch_loss:0.0016 \n",
      "   epoch:   19 , batch:  750 , avg_batch_acc:0.9616 , avg_batch_loss:0.0017 \n",
      "   epoch:   19 , batch:  800 , avg_batch_acc:0.9675 , avg_batch_loss:0.0017 \n",
      "   epoch:   19 , batch:  850 , avg_batch_acc:0.9691 , avg_batch_loss:0.0015 \n",
      "   epoch:   19 , batch:  900 , avg_batch_acc:0.9775 , avg_batch_loss:0.0012 \n",
      "    **********epoch:   19 , avg_epoch_acc:0.9710 , avg_epoch_loss:0.0015 *************\n",
      "------------test_set_acc:0.9713---------------\n",
      "   epoch:   20 , batch:   50 , avg_batch_acc:0.9731 , avg_batch_loss:0.0014 \n",
      "   epoch:   20 , batch:  100 , avg_batch_acc:0.9738 , avg_batch_loss:0.0013 \n",
      "   epoch:   20 , batch:  150 , avg_batch_acc:0.9659 , avg_batch_loss:0.0016 \n",
      "   epoch:   20 , batch:  200 , avg_batch_acc:0.9731 , avg_batch_loss:0.0014 \n",
      "   epoch:   20 , batch:  250 , avg_batch_acc:0.9675 , avg_batch_loss:0.0016 \n",
      "   epoch:   20 , batch:  300 , avg_batch_acc:0.9769 , avg_batch_loss:0.0013 \n",
      "   epoch:   20 , batch:  350 , avg_batch_acc:0.9753 , avg_batch_loss:0.0013 \n",
      "   epoch:   20 , batch:  400 , avg_batch_acc:0.9750 , avg_batch_loss:0.0012 \n",
      "   epoch:   20 , batch:  450 , avg_batch_acc:0.9703 , avg_batch_loss:0.0016 \n",
      "   epoch:   20 , batch:  500 , avg_batch_acc:0.9712 , avg_batch_loss:0.0014 \n",
      "   epoch:   20 , batch:  550 , avg_batch_acc:0.9747 , avg_batch_loss:0.0014 \n",
      "   epoch:   20 , batch:  600 , avg_batch_acc:0.9691 , avg_batch_loss:0.0016 \n",
      "   epoch:   20 , batch:  650 , avg_batch_acc:0.9712 , avg_batch_loss:0.0015 \n",
      "   epoch:   20 , batch:  700 , avg_batch_acc:0.9741 , avg_batch_loss:0.0015 \n",
      "   epoch:   20 , batch:  750 , avg_batch_acc:0.9634 , avg_batch_loss:0.0017 \n",
      "   epoch:   20 , batch:  800 , avg_batch_acc:0.9691 , avg_batch_loss:0.0016 \n",
      "   epoch:   20 , batch:  850 , avg_batch_acc:0.9700 , avg_batch_loss:0.0015 \n",
      "   epoch:   20 , batch:  900 , avg_batch_acc:0.9784 , avg_batch_loss:0.0012 \n",
      "    **********epoch:   20 , avg_epoch_acc:0.9717 , avg_epoch_loss:0.0014 *************\n",
      "------------test_set_acc:0.9718---------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 64  \n",
    "test_batch = 50  \n",
    "epoch = 20\n",
    "learning_rate = 1e-3\n",
    "\n",
    "iterations_num = 0 \n",
    "\n",
    "net = LeNet5()\n",
    "\n",
    "for E in range(epoch):\n",
    "    batch_loss = 0\n",
    "    batch_acc = 0\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    for i in range(train_images.shape[0] // batch_size):\n",
    "        img = train_images[i*batch_size:(i+1)*batch_size].reshape(batch_size, 1, 28, 28)\n",
    "        img = normalization(img)\n",
    "        label = train_labels[i*batch_size:(i+1)*batch_size]\n",
    "        loss, prediction = net.forward(img, label, is_train=True) \n",
    "\n",
    "        epoch_loss += loss\n",
    "        batch_loss += loss\n",
    "        for j in range(prediction.shape[0]):\n",
    "            if np.argmax(prediction[j]) == label[j]:\n",
    "                epoch_acc += 1\n",
    "                batch_acc += 1\n",
    "\n",
    "        net.backward(learning_rate)\n",
    "\n",
    "        if (i+1)%50 == 0:\n",
    "            print(\"   epoch:%5d , batch:%5d , acc:%.4f , loss:%.4f \"\n",
    "                  % (E+1, i+1, batch_acc/(batch_size*50), batch_loss/(batch_size*50)))\n",
    "            \n",
    "\n",
    "            batch_loss = 0\n",
    "            batch_acc = 0\n",
    "\n",
    "\n",
    "\n",
    "    print(\"--------------epoch:%5d , avg_acc:%.4f , avg_loss:%.4f---------------\"\n",
    "          % (E+1, epoch_acc/train_images.shape[0], epoch_loss/train_images.shape[0]))\n",
    "    test_acc = 0\n",
    "    for k in range(test_images.shape[0] // test_batch):\n",
    "        img = test_images[k*test_batch:(k+1)*test_batch].reshape(test_batch, 1 ,28, 28)\n",
    "        img = normalization(img)\n",
    "        label = test_labels[k*test_batch:(k+1)*test_batch]\n",
    "        _, prediction = net.forward(img, label, is_train=False)\n",
    "\n",
    "        for j in range(prediction.shape[0]):\n",
    "            if np.argmax(prediction[j]) == label[j]:\n",
    "                test_acc += 1\n",
    "\n",
    "    print(\"------------total_acc:%.4f---------------\" % (test_acc / test_images.shape[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
